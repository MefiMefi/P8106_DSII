---
title: "Resampling Methods for Assessing Model Accuracy" 
author: "Yifei Sun"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
--- 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```


\newpage

```{r}
library(FNN)
library(caret)
```


You can generate a simulated training dataset or use an existing dataset. For illustration, we use a simulated dataset with two predictors.
```{r}
# Data generating function - you can replace this with your own function
gen_data <- function(N)
{
  X1 <- rnorm(N, mean = 1)
  X2 <- rnorm(N, mean = 1)
  eps <- rnorm(N, sd = .5)
  Y <- sin(X1) + (X2)^2 + eps
  data.frame(Y = Y, X1 = X1, X2 = X2)
}

set.seed(2022)

# generate the *training* data
N <- 200
trainData <- gen_data(N)
```


# Data splitting functions

## Training/Validation splitting

Sampling in `createDataPartition()`: For factor `y` (e.g., classification), the random sampling is done within the levels of `y` in an attempt to balance the class distributions within the splits. For numeric `y`, the sample is split into groups sections based on percentiles and sampling is done within these subgroups. 

```{r}
vsSplits <- createDataPartition(y = trainData$Y,
                                times = 2,
                                p = 0.8,
                                groups = 5,
                                list = FALSE)

head(vsSplits)
```

## (Repeated) K-fold CV

Sometimes we can repeat the K-fold CV multiple times and then calculate the average prediction error.

```{r}
# ten-fold CV 
set.seed(1)
cvSplits <- createFolds(y = trainData$Y, 
                        k = 10, 
                        returnTrain = T)
                        #returnTrain = FALSE)

str(cvSplits)

# repeated ten-fold CV 
set.seed(1)
rcvSplits <- createMultiFolds(y = trainData$Y, 
                              k = 10, 
                              times = 5)
# Foldi.Repj - the ith section (of k) of the jth cross-validation set 
length(rcvSplits)
str(rcvSplits)
```

A simple example:

```{r}
K <- length(rcvSplits)
mseK_lm <- rep(NA, K)
mseK_knn <- rep(NA, K)

for(k in 1:K)
{
  trRows <- rcvSplits[[k]]
  
  fit_lm <- lm(Y~X1+X2, data = trainData[trRows,])
  pred_lm <- predict(fit_lm, trainData[-trRows,])
  
  pred_knn <- knn.reg(train = trainData[trRows,2:3], 
                      test = trainData[-trRows,2:3], 
                      y = trainData$Y[trRows], k = 3)
  
  
  mseK_lm[k] <- mean((trainData$Y[-trRows] - pred_lm)^2)
  mseK_knn[k] <- mean((trainData$Y[-trRows] - pred_knn$pred)^2)
}

c(mean(mseK_lm), mean(mseK_knn))
```


# Specify the resampling method using `trainControl()`

All the resampling methods in the slides are available in `trainControl()`.

```{r}
# K-fold CV
ctrl1 <- trainControl(method = "cv", number = 10)
# leave-one-out CV
ctrl2 <- trainControl(method = "LOOCV")
# leave-group-out / Monte Carlo CV
ctrl3 <- trainControl(method = "LGOCV", p = 0.75, number = 50) 
# 632 bootstrap
ctrl4 <- trainControl(method = "boot632", number = 100)
# repeated K-fold CV
ctrl5 <- trainControl(method = "repeatedcv", repeats = 5, number = 10) 
# user-specified folds 
ctrl7 <- trainControl(index = rcvSplits)


# only fit one model to the entire training set
ctrl6 <- trainControl(method = "none") 



set.seed(1)
lmFit <- train(Y~., 
               data = trainData, 
               method = "lm", 
               trControl = ctrl4)


set.seed(1)
knnFit <- train(Y~., 
                data = trainData, 
                method = "knn", 
                trControl = ctrl4)

# same training/validation splits?
identical(lmFit$control$index, knnFit$control$index)

# compare with mean(mseK_lm) above
lmFit2 <- train(Y~., 
                data = trainData, 
                method = "lm", 
                trControl = ctrl7)

mean((lmFit2$resample$RMSE)^2)
mean(mseK_lm)

knnFit2 <- train(Y~., 
                 data = trainData, 
                 method = "knn", 
                 tuneGrid = data.frame(k = 3),
                 trControl = ctrl7)

mean((knnFit2$resample$RMSE)^2)
mean(mseK_knn)
```

To compare these two models based on their cross-validation statistics, the `resamples()` function can be used with models that share a *common* set of resampled datasets.

```{r}
resamp <- resamples(list(lm = lmFit, knn = knnFit))
summary(resamp)
```



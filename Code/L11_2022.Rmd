---
title: "Regression Trees and Classification Trees"
author: "Yifei Sun"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
--- 

\newpage


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
library(ISLR)
library(mlbench)
library(caret)
library(rpart)
library(rpart.plot)
library(party)
library(partykit)
library(pROC)
```



# Regression Trees

Predict a baseball player’s salary on the basis of various statistics associated with performance in the previous year. Use `?Hitters` for more details.

```{r}
data(Hitters)
Hitters <- na.omit(Hitters)

set.seed(2021)
trRows <- createDataPartition(Hitters$Salary,
                              p = .75,
                              list = F)
```

## The CART approach

We first apply the regression tree method to the Hitters data. `cp` is the complexity parameter. The default value for `cp` is 0.01. Sometimes the default value may over prune the tree.

`rpart()` represent "recursive partition". `rpart()` doesn't return the pruned tree, although it do did cross-validation. `cp` is the tuning parameter.

The loss function in `rpart()` is:
$$\frac{RSS}{TSS} + cp\times|T| = 1-R^2 +cp\times|T|$$
So `cp` is less than 1.
$$
cp=1 \ \rightarrow \text{Root Node} \\
cp = 0 \ \rightarrow T_0
$$
```{r}
set.seed(1)
tree1 <- rpart(formula = Salary ~ . , 
               data = Hitters, subset = trRows,
               control = rpart.control(cp = 0))

## plot.rpart
# plot(tree1)
# text(tree1)

rpart.plot(tree1)
```



We get a smaller tree by increasing the complexity parameter.(**Not Recommended**, just for illustration!)
```{r, fig.height=3, fig.width=4}
set.seed(1)
tree2 <- rpart(Salary ~ . ,  
               data = Hitters, subset = trRows,
               control = rpart.control(cp = 0.1))
rpart.plot(tree2)
```


We next apply cost complexity pruning to obtain a tree with the right size. The functions `printcp()` and `plotcp()` give the set of possible cost-complexity prunings of a tree from a nested set. For the geometric means of the intervals of values of `cp` for which a pruning is optimal, a cross-validation has been done in the initial construction by `rpart()`. 

The `cptable` in the fit contains the mean and standard deviation of the errors in the cross-validated prediction against each of the geometric means, and these are plotted by `plotcp()`. `Rel error` (relative error) is `\(1 – R^2\)`. The x-error is the cross-validation error generated by built-in cross validation. A good choice of `cp` for pruning is often the leftmost value for which the mean lies below the horizontal line.
```{r}
printcp(tree1)
cpTable <- tree1$cptable
plotcp(tree1)
```
The x-axis cp is the geometric mean, the best cp is derived from `sqrt(0.06250035*0.1280512)`. The plot only shows the represetative value.


Prune the tree based on the `cp` table. Look at the `xerror`!!!

```{r}
# minimum cross-validation error
minErr <- which.min(cpTable[,4])
tree3 <- prune(tree1, cp = cpTable[minErr,1])
rpart.plot(tree3)
plot(as.party(tree3))
summary(tree3)

with(Hitters[trRows,], table(cut(CRBI, c(-Inf, 307.5, Inf)),
                             cut(CAtBat, c(-Inf, 2316.5, Inf))))
```


```{r}
# 1SE rule
tree4 <- prune(tree1, cp = cpTable[cpTable[,4]<cpTable[minErr,4]+cpTable[minErr,5],1][1])
rpart.plot(tree4)
```

Finally, the function `predict()` can be used for prediction from a fitted `rpart` object.

```{r}
head(predict(tree3, newdata = Hitters[-trRows,]))
```

### Missing data

```{r}
Hitters2 <- Hitters
Hitters2$CRBI[sample(1:nrow(Hitters2), 50)] <- NA

set.seed(1)
tree_m <- rpart(Salary ~ . ,  
                data = Hitters2,
                subset = trRows,
                control = rpart.control(cp = 0))

cpTable_m <- tree_m$cptable
tree2_m <- prune(tree_m, cp = cpTable_m[which.min(cpTable_m[,4]),1])

summary(tree_m, cp = cpTable_m[which.min(cpTable_m[,4]),1])
head(predict(tree2_m, newdata = Hitters2[-trRows,]))
```

## Conditional inference trees

The implementation utilizes a unified framework for conditional inference, or permutation tests. Unlike CART, the stopping criterion is based on p-values. A split is implemented when (1 - p-value) exceeds the value given by `mincriterion` as specified in `ctree_control()`. This approach ensures that the right-sized tree is grown without additional pruning or cross-validation, but can stop early. At each step, the splitting variable is selected as the input variable with strongest association to the response (measured by a p-value corresponding to a test for the partial null hypothesis of a single input variable and the response). Such a splitting procedure can avoid a variable selection bias towards predictors with many possible cutpoints.

```{r, fig.height=4, fig.width=8}
tree5 <- ctree(Salary ~ . , Hitters,
               subset = trRows)
plot(tree5)
```

Note that `tree5` is a `party` object. The function `predict()` can be used for prediction from a fitted `party` object.

```{r}
head(predict(tree5, newdata = Hitters[-trRows,]))
```

## `caret`


```{r}
ctrl <- trainControl(method = "cv")

set.seed(1)
rpart.fit <- train(Salary ~ . , 
                   Hitters[trRows,], 
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-6,-2, length = 50))),
                   trControl = ctrl)
ggplot(rpart.fit, highlight = TRUE)
rpart.plot(rpart.fit$finalModel)
```

We can also fit a conditional inference tree model. The tuning parameter is `mincriterion`.

```{r, fig.height=4, fig.width=8}
set.seed(1)
ctree.fit <- train(Salary ~ . , 
                   Hitters[trRows,], 
                   method = "ctree",
                   tuneGrid = data.frame(mincriterion = 1-exp(seq(-6, -2, length = 50))),
                   trControl = ctrl)
ggplot(ctree.fit, highlight = TRUE)
plot(ctree.fit$finalModel)
```


```{r}
summary(resamples(list(rpart.fit, ctree.fit)))
```

```{r}
RMSE(predict(rpart.fit, newdata = Hitters[-trRows,]), Hitters$Salary[-trRows])
RMSE(predict(ctree.fit, newdata = Hitters[-trRows,]), Hitters$Salary[-trRows])
```

# Classification trees

We use the Pima Indians Diabetes Database for illustration. The data contain 768 observations and 9 variables. The outcome is a binary variable `diabetes`. 

```{r}
data(PimaIndiansDiabetes)
dat <- PimaIndiansDiabetes
dat$diabetes <- factor(dat$diabetes, c("pos", "neg"))

set.seed(2)
rowTrain <- createDataPartition(y = dat$diabetes,
                                p = 2/3,
                                list = FALSE)
```


## `rpart`

```{r}
set.seed(1)
tree1 <- rpart(formula = diabetes ~ . , 
               data = dat,
               subset = rowTrain, 
               control = rpart.control(cp = 0))

cpTable <- printcp(tree1)
plotcp(tree1)

# minimum cross-validation error; may also use the 1SE rule
minErr <- which.min(cpTable[,4])
tree2 <- prune(tree1, cp = cpTable[minErr,1])
rpart.plot(tree2)
summary(tree2)
```

## `ctree`

```{r, fig.height=6, fig.width=10}
tree2 <- ctree(formula = diabetes ~ . , 
               data = dat,
               subset = rowTrain)
plot(tree2)
```
 
## `caret`

### CART

```{r}
ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

set.seed(1)
rpart.fit <- train(diabetes ~ . , 
                   dat, 
                   subset = rowTrain,
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-6,-3, len = 50))),
                   trControl = ctrl,
                   metric = "ROC")
ggplot(rpart.fit, highlight = TRUE)
rpart.plot(rpart.fit$finalModel)
```

### CIT

```{r}
set.seed(1)
ctree.fit <- train(diabetes ~ . , dat, 
                   subset = rowTrain,
                   method = "ctree",
                   tuneGrid = data.frame(mincriterion = 1-exp(seq(-2, -1, length = 50))),
                   metric = "ROC",
                   trControl = ctrl)
ggplot(ctree.fit, highlight = TRUE)
```

```{r, fig.width=15, fig.height=6}
plot(ctree.fit$finalModel)
summary(resamples(list(rpart.fit, ctree.fit)))
```



```{r}
rpart.pred <- predict(tree1, newdata = dat[-rowTrain,])[,1]

rpart.pred2 <- predict(rpart.fit, newdata = dat[-rowTrain,],
                       type = "prob")[,1]

ctree.pred <- predict(ctree.fit, newdata = dat[-rowTrain,],
                       type = "prob")[,1]

roc.rpart <- roc(dat$diabetes[-rowTrain], rpart.pred2)
roc.ctree <- roc(dat$diabetes[-rowTrain], ctree.pred)

auc <- c(roc.rpart$auc[1], roc.ctree$auc[1])

plot(roc.rpart, legacy.axes = TRUE)
plot(roc.ctree, col = 2, add = TRUE)


modelNames <- c("rpart","ctree")
legend("bottomright", legend = paste0(modelNames, ": ", round(auc,3)),
       col = 1:2, lwd = 2)
```




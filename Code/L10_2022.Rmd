---
title: "Data Preprocessing"
author: "Yifei Sun"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
--- 

\newpage
\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
library(caret)
library(visdat)
library(gridExtra)
library(mvtnorm)
library(ISLR)
```

# Transforming predictors

Although not always required, transforming the variables may lead to improvement in prediction, especially for parametric models.

For example, one may consider the Box-Cox transformation, which finds an appropriate transformation from a family of power transformation that will transform the variable as close as possible to a normal distribution. One may also consider the Yeo-Johnson transformation if the variables are not strictly positive.

```{r}
gen_data <- function(N)
{
  X <- rmvnorm(N, mean = c(1,-1), 
               sigma = matrix(c(1,.5,.5,1), ncol = 2))# sigma is the vcov martix
  X1 <- exp(X[,1]) # X1 follows log normal distribution
  X2 <- X[,2] # X2 follows normal
  X3 <- rep(1, N) # useless same value for all
  eps <- rnorm(N, sd = .5) 
  Y <- log(X1) + X2 + eps
  
  data.frame(Y = Y, X1 = X1, X2 = X2, X3 = X3)
}

set.seed(2022)
trainData <- gen_data(200)
testData <- gen_data(100)

x <- trainData[, -1]
y <- trainData[, 1]
x2 <- testData[, -1]
y2 <- testData[, 1]
```

## `preProcess` in `train()`

- Preprocess should only be done in train data, which actually a part of model building where we do $x \rightarrow g(x) \rightarrow f(g(x))$. 

- Resamping should be done only in training set.

- Avoiding data leaking.

```{r}
fit.lm <- train(x, y,
                preProcess = c("BoxCox", "zv"), # "zv" means zero variance, try  "nzv/nearzv". Box-cox is automatically done
                method = "lm",
                trControl = trainControl(method = "none"))# you can change it to cv

fit.lm

pred.lm <- predict(fit.lm, newdata = x2)

fit.lm$preProcess$bc
#fit.lm$preProcess

#OUTPUT:
#Created from 200 samples and 2 variables
#
#Pre-processing:
#  - Box-Cox transformation (1)
#  - ignored (0)
#  - removed (1)
#
#Lambda estimates for Box-Cox transformation:
#0
```

- When estimated $\lambda = 0.1$, fudge factor set to be 0.2 or 0.3 can set $\lambda \rightarrow 0$

- `predict()` do the `preProcess()` automatically on test data.

## `preProcess()`

The transformation is computed using the training data. Then it is applied to both training and test data.

```{r}
pp <- preProcess(x, method = c("BoxCox", "zv"))

# transformed predictor matrix (training)
x_pp <- predict(pp, x) # predict.preProcess just used to transform predictors NOT doing predictions

head(x_pp) # thrid column disappeare

# transformed predictor matrix (test)
x2_pp <- predict(pp, x2)

head(x2_pp)
```


# Missing data

There are different mechanisms for missing data: missing completely at random (MCAR), missing at random (MAR), missing not at random (MNAR). MAR means that the missingness depends
only on the observed data; MNAR means that the missingness further depends on the missing data. The missing data mechanism determines how you handle the missing data. For example, under MAR, you may consider imputation methods; under MNAR, you might consider treating missingness as an attribute.


```{r}
gen_data <- function(N)
{
  X <- rmvnorm(N, mean = c(1,-1), 
               sigma = matrix(c(1,0.5,0.5,1), ncol = 2))
  X1 <- X[,1]
  X2 <- X[,2]
  eps <- rnorm(N, sd = .5)
  Y <- X1 + X2 + eps
  
  # which X1 observations are missing
  ind_missing <- rbinom(N, size = 1, prob = exp(X2/2)/(1+exp(X2/2)))
  
  X1m <- X1
  X1m[ind_missing == 1] <- NA
  
  data.frame(Y = Y, X1m = X1m, X2 = X2, X1 = X1)
}

set.seed(2022)

dat <- gen_data(1000)
dat2 <- gen_data(200)
trainData <- dat[,1:3]
testData <- dat2[,1:3]

vis_miss(trainData)
```



## `preProcess()`

```{r}
trainX <- trainData[,c(2:3)]
knnImp <- preProcess(trainX, method = "knnImpute", k = 3)
bagImp <- preProcess(trainX, method = "bagImpute")
medImp <- preProcess(trainX, method = "medianImpute")

trainX_knn <- predict(knnImp, trainX)
trainX_bag <- predict(bagImp, trainX)
trainX_med <- predict(medImp, trainX)

testData_knn <- predict(knnImp, testData)
testData_bag <- predict(bagImp, testData)
testData_med <- predict(medImp, testData)

head(trainX)

head(trainX_med)

head(trainX_knn)

head(trainX_bag)
```


```{r, echo = FALSE}
df <- data.frame(X1 = dat$X1, 
                 X1med = trainX_med$X1m,
                 X1knn = trainX_knn$X1m,
                 X1bag = trainX_bag$X1m,
                 Y = trainData$Y,
                 X2 = trainData$X2)[is.na(trainData$X1m),]

p1 <- ggplot(df, aes(x = X1, y = X2, color = Y)) + 
  geom_point(show.legend = TRUE) +
  labs(x = "X1", y = "X2",  title = "No missingness", color = "Y") + 
  scale_color_gradient(low = "blue", high = "red") +
  theme_minimal() + xlim(c(-3,4.2)) + ylim(c(-4.5,4)) +
  geom_point(data = dat[!is.na(trainData$X1m),], 
             mapping = aes(x = X1, y = X2),
             colour = "grey", alpha = 0.2)

p2 <- ggplot(df, aes(x = X1med, y = X2, color = Y)) + 
  geom_point(show.legend = TRUE) +
  labs(x = "X1", y = "X2",  title = "medianImpute", color = "Y") + 
  scale_color_gradient(low = "blue", high = "red") +
  theme_minimal() + xlim(c(-3,4.2)) + ylim(c(-4.5,4))

p3 <- ggplot(df, aes(x = X1knn, y = X2, color = Y)) + 
  geom_point(show.legend = TRUE) +
  labs(x = "X1", y = "X2",  title = "knnImpute", color = "Y") + 
  scale_color_gradient(low = "blue", high = "red") +
  theme_minimal() + xlim(c(-3,4.2)) + ylim(c(-4.5,4))

p4 <- ggplot(df, aes(x = X1bag, y = X2, color = Y)) + 
  geom_point(show.legend = TRUE) +
  labs(x = "X1", y = "X2",  title = "bagImpute", color = "Y") + 
  scale_color_gradient(low = "blue", high = "red") +
  theme_minimal() + xlim(c(-3,4.2)) + ylim(c(-4.5,4))

grid.arrange(p1, p2, p3, p4)
```



## `preProcess` in `train()`

```{r}
fit.lm <- train(x = trainData[,c(2,3)],
                y = trainData$Y,
                preProcess = c("knnImpute"), # bagImpute/medianImpute
                method = "lm",
                trControl = trainControl(method = "none",
                                         preProcOptions = list(k = 5)))

pred.lm <- predict(fit.lm, newdata = testData)

mean((testData$Y - pred.lm)^2)

# Imputation is performed within the resampling process
fit.lm2 <- train(x = trainData[,c(2,3)],
                 y = trainData$Y,
                 preProcess = c("knnImpute"), 
                 method = "lm",
                 trControl = trainControl(method = "cv",
                                          preProcOptions = list(k = 5)))
```



